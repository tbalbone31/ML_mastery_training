# %%
from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LogisticRegression

import matplotlib.pyplot as plt
import numpy as np


url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(url, names=names)
X = dataframe.loc[:,'preg':'age']
Y = dataframe.loc[:,'class']

# %%
kfold = KFold(n_splits=10)
model = LogisticRegression(solver='liblinear')

# Many estimators have their own internal score method, this can be overidden when calculating metrics.

# %%
# Cross Validation Classification LogLoss
# metrics using logistic loss or cross-entropy loss https://scikit-learn.org/stable/modules/model_evaluation.html#log-loss
scoring = 'neg_log_loss'
results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print("Logloss: %.3f (%.3f)" % (results.mean(), results.std()))

#%%
# Cross Validation Classification Accuracy
# metrics using accuracy https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score
scoring = 'accuracy'
results = cross_val_score(model, X, Y, cv=kfold, scoring=scoring)
print("Accuracy: %.3f (%.3f)" % (results.mean(), results.std()))
# %%
# Confusion Matrix and Classification report
from sklearn.metrics import ConfusionMatrixDisplay, classification_report
from sklearn.model_selection import train_test_split

# Split the data into a training set and a test set
X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=0)

classifier = model.fit(X_train,y_train)

y_pred = model.predict(X_test)

np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
titles_options = [
    ("Confusion matrix, without normalization", None),
    ("Normalized confusion matrix", "true"),
]
for title, normalize in titles_options:
    disp = ConfusionMatrixDisplay.from_estimator(
        classifier,
        X_test,
        y_test,
        cmap=plt.cm.Blues,
        normalize=normalize,
    )
    disp.ax_.set_title(title)

    print(title)
    print(disp.confusion_matrix)

plt.show()

print(classification_report(y_test,y_pred))

# %%
# Practice using RMSE and RSquared metrics on a regression problem

# The Problem:  To analyse wine data to predict the quality of Red Wine given certain variables.
# The dataset: UCI Wine Quality dataset http://archive.ics.uci.edu/dataset/186/wine+quality
# To see a data dictionary https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009

# First import relevant libraries

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn as sk
import seaborn as sns
import statsmodels.api as sm
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, root_mean_squared_error
from sklearn.model_selection import train_test_split


# Load and Read in the data
red_wine_raw_df = pd.read_csv("winequality-red.csv", sep=";")

# Take a random sample

red_wine_raw_df

red_wine_X = red_wine_raw_df.loc[:,'fixed acidity':'alcohol']
red_wine_Y = red_wine_raw_df.loc[:,'quality']

# Split the data into a training set and a test set
red_wine_X_train, red_wine_X_test, red_wine_y_train, red_wine_y_test = train_test_split(red_wine_X, red_wine_Y, random_state=0)

red_wine_reg_model = LinearRegression()
red_wine_reg_model.fit(red_wine_X_train,red_wine_y_train)

red_wine_y_pred = red_wine_reg_model.predict(red_wine_X_test)

mse = mean_squared_error(red_wine_y_test, red_wine_y_pred)
rmse = root_mean_squared_error(red_wine_y_test, red_wine_y_pred)

print(f"Mean Squared Error: {mse} Root Mean Squared Error: {rmse}")

# %%
